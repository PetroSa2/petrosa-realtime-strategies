# Business Metrics for Realtime Strategies

## Overview

This document describes the custom business metrics added to the petrosa-realtime-strategies service for monitoring real-time strategy performance, message processing, and signal generation.

## Metrics

### 1. Message Processing Counter

**Metric Name**: `realtime.messages.processed`  
**Type**: Counter  
**Unit**: count (1)

**Description**: Tracks the total number of messages processed by the realtime strategies service.

**Labels**:
- `symbol`: Trading symbol (e.g., "BTCUSDT", "ETHUSDT")
- `message_type`: Type of message processed ("depth", "trade", "ticker")
- `strategy`: Strategy name (optional)

**Example PromQL Queries**:
```promql
# Message processing rate (messages/sec)
rate(realtime_messages_processed_total[1m])

# Messages by symbol
sum by(symbol) (rate(realtime_messages_processed_total[5m]))

# Messages by type
sum by(message_type) (rate(realtime_messages_processed_total[5m]))
```

### 2. Strategy Execution Latency

**Metric Name**: `realtime.strategy.latency`  
**Type**: Histogram  
**Unit**: milliseconds (ms)

**Description**: Measures the execution time for each strategy from start to finish (including signal generation).

**Labels**:
- `strategy`: Strategy name ("spread_liquidity", "iceberg_detector", "btc_dominance", etc.)
- `symbol`: Trading symbol

**Example PromQL Queries**:
```promql
# 95th percentile latency
histogram_quantile(0.95, rate(realtime_strategy_latency_bucket[1m]))

# Average latency by strategy
rate(realtime_strategy_latency_sum[1m]) / rate(realtime_strategy_latency_count[1m])

# Maximum latency
histogram_quantile(1.0, rate(realtime_strategy_latency_bucket[1m]))
```

### 3. NATS Consumer Lag

**Metric Name**: `realtime.consumer.lag`  
**Type**: Gauge (Observable)  
**Unit**: seconds (s)

**Description**: Measures the time difference between when a message was created and when it was processed. Indicates processing backlog.

**Labels**: None

**Example PromQL Queries**:
```promql
# Current consumer lag
realtime_consumer_lag

# Average lag over 5 minutes
avg_over_time(realtime_consumer_lag[5m])
```

**Thresholds**:
- **Normal**: < 5 seconds
- **Warning**: 5-30 seconds
- **Critical**: > 30 seconds

### 4. Signals Generated Counter

**Metric Name**: `realtime.signals.generated`  
**Type**: Counter  
**Unit**: count (1)

**Description**: Tracks signals generated by each strategy (buy/sell/hold decisions).

**Labels**:
- `strategy`: Strategy name
- `signal_type`: Signal action ("buy", "sell", "hold")
- `symbol`: Trading symbol
- `confidence_bucket`: Confidence level ("very_high", "high", "medium", "low")

**Confidence Buckets**:
- **very_high**: confidence >= 0.9
- **high**: 0.75 <= confidence < 0.9
- **medium**: 0.5 <= confidence < 0.75
- **low**: confidence < 0.5

**Example PromQL Queries**:
```promql
# Signal generation rate
rate(realtime_signals_generated_total[1m])

# Signals by type
sum by(signal_type) (rate(realtime_signals_generated_total[5m]))

# High confidence signals only
sum(rate(realtime_signals_generated_total{confidence_bucket="very_high"}[5m]))

# Signals by strategy
sum by(strategy) (rate(realtime_signals_generated_total[5m]))
```

### 5. Strategy Execution Counter

**Metric Name**: `realtime.strategy.executions`  
**Type**: Counter  
**Unit**: count (1)

**Description**: Tracks strategy execution results (success, failure, no signal).

**Labels**:
- `strategy`: Strategy name
- `result`: Execution result ("success", "failure", "no_signal")
- `symbol`: Trading symbol

**Example PromQL Queries**:
```promql
# Strategy success rate
rate(realtime_strategy_executions_total{result="success"}[1m]) / 
rate(realtime_strategy_executions_total[1m])

# Failed executions
rate(realtime_strategy_executions_total{result="failure"}[1m])

# No signal rate (strategy executed but no actionable signal)
rate(realtime_strategy_executions_total{result="no_signal"}[1m])
```

### 6. Error Counter

**Metric Name**: `realtime.errors.total`  
**Type**: Counter  
**Unit**: count (1)

**Description**: Tracks errors during message processing and strategy execution.

**Labels**:
- `error_type`: Type of error ("invalid_message", "message_processing", "strategy_execution", "microstructure_processing", "market_logic_processing")
- `strategy`: Strategy name (optional)

**Example PromQL Queries**:
```promql
# Error rate
rate(realtime_errors_total[1m])

# Errors by type
sum by(error_type) (rate(realtime_errors_total[5m]))

# Errors by strategy
sum by(strategy) (rate(realtime_errors_total{strategy!=""}[5m]))
```

### 7. Message Types Counter

**Metric Name**: `realtime.message.types`  
**Type**: Counter  
**Unit**: count (1)

**Description**: Tracks messages received by type (depth/trade/ticker).

**Labels**:
- `type`: Message type ("depth", "trade", "ticker", "unknown")

**Example PromQL Queries**:
```promql
# Message type distribution
sum by(type) (rate(realtime_message_types_total[5m]))

# Depth messages only
rate(realtime_message_types_total{type="depth"}[1m])
```

## Grafana Dashboard

A pre-built Grafana dashboard is available at `docs/grafana-dashboard.json`.

### Dashboard Panels

1. **Message Processing Rate**: Line chart showing messages/sec by symbol and type
2. **Strategy Execution Latency (p95)**: 95th percentile latency for each strategy
3. **NATS Consumer Lag**: Current consumer lag with thresholds
4. **Signals Generated (by Type)**: Signal generation rate by strategy and type
5. **Strategy Success Rate**: Percentage of successful strategy executions
6. **Error Rate**: Error rate by type and strategy
7. **Message Types Distribution**: Pie chart of message type breakdown
8. **Signal Confidence Distribution**: Pie chart of signal confidence levels
9. **Top Strategies by Signal Count**: Bar chart of most active strategies
10. **Strategy Execution Count**: Stat panel showing total execution rate
11. **Total Messages Processed**: Stat panel showing message processing rate
12. **Average Strategy Latency**: Average latency over time by strategy

### Importing the Dashboard

1. Open Grafana
2. Navigate to Dashboards â†’ Import
3. Upload `docs/grafana-dashboard.json`
4. Select your Prometheus data source
5. Click Import

## Usage in Code

### Automatic Instrumentation

The metrics are automatically instrumented in the NATS consumer (`strategies/core/consumer.py`):

- Message processing is tracked in `_process_message()`
- Strategy execution is tracked using `MetricsContext` context manager
- Errors are automatically recorded on exception

### Manual Instrumentation

You can also manually record metrics in custom strategies:

```python
from strategies.utils.metrics import get_metrics, MetricsContext

# Get global metrics instance
metrics = get_metrics()

# Record message processed
metrics.record_message_processed(symbol="BTCUSDT", message_type="depth")

# Use context manager for strategy timing
with MetricsContext(strategy="my_strategy", symbol="BTCUSDT", metrics=metrics) as ctx:
    # Process strategy logic
    signal = process_my_strategy(data)
    
    if signal:
        # Record signal
        ctx.record_signal(signal.signal_action, signal.confidence)

# Record errors
metrics.record_error("custom_error", strategy="my_strategy")
```

## Alerting Rules

### Recommended Prometheus Alerts

```yaml
groups:
  - name: realtime_strategies_alerts
    interval: 1m
    rules:
      - alert: HighConsumerLag
        expr: realtime_consumer_lag > 30
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "NATS consumer lag is high"
          description: "Consumer lag is {{ $value }}s (threshold: 30s)"

      - alert: HighStrategyLatency
        expr: histogram_quantile(0.95, rate(realtime_strategy_latency_bucket[5m])) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Strategy execution latency is high"
          description: "P95 latency is {{ $value }}ms (threshold: 100ms)"

      - alert: HighErrorRate
        expr: rate(realtime_errors_total[5m]) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Error rate is elevated"
          description: "Error rate is {{ $value }}/s (threshold: 1/s)"

      - alert: LowSignalGeneration
        expr: rate(realtime_signals_generated_total[10m]) < 0.01
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Signal generation rate is low"
          description: "Only {{ $value }} signals/s generated in last 10min"

      - alert: LowStrategySuccessRate
        expr: |
          rate(realtime_strategy_executions_total{result="success"}[5m]) /
          rate(realtime_strategy_executions_total[5m]) < 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Strategy success rate is low"
          description: "Success rate is {{ $value | humanizePercentage }} (threshold: 50%)"
```

## Performance Impact

The OpenTelemetry metrics are designed to have minimal performance impact:

- **Counters**: O(1) atomic increment
- **Histograms**: O(1) bucket lookup
- **Gauges**: O(1) value update
- **Context Manager**: ~0.1ms overhead per strategy execution

**Estimated overhead**: < 1% additional CPU usage for typical workload (1000 msg/sec).

## Troubleshooting

### Metrics Not Appearing in Grafana

1. Verify OpenTelemetry is initialized:
   ```bash
   kubectl logs -n petrosa-apps deployment/petrosa-realtime-strategies | grep "Custom business metrics initialized"
   ```

2. Check metrics are being exported:
   ```bash
   kubectl logs -n petrosa-apps deployment/petrosa-realtime-strategies | grep "realtime\."
   ```

3. Verify Prometheus is scraping the metrics endpoint:
   ```bash
   curl http://petrosa-realtime-strategies:9090/metrics | grep realtime_
   ```

### High Consumer Lag

**Possible Causes**:
- High message volume exceeding processing capacity
- Slow strategy execution
- Resource constraints (CPU/memory throttling)

**Solutions**:
1. Scale horizontally (increase replicas)
2. Optimize strategy logic
3. Increase resource limits
4. Disable non-critical strategies

### High Strategy Latency

**Possible Causes**:
- Expensive computations in strategy
- Blocking I/O operations
- Database query slowness

**Solutions**:
1. Profile strategy code
2. Add caching for expensive operations
3. Use async/await properly
4. Optimize database queries

## References

- OpenTelemetry Metrics API: https://opentelemetry.io/docs/instrumentation/python/manual/#metrics
- Prometheus Query Examples: https://prometheus.io/docs/prometheus/latest/querying/examples/
- Grafana Dashboard Best Practices: https://grafana.com/docs/grafana/latest/dashboards/

